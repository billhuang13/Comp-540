# -*- coding: utf-8 -*-
"""Classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17BwAnGV33pBxnZHp0xfpGWXvARcDaJGI
"""

from torch.utils.data import Dataset, DataLoader
import cv2
import sys
from glob import glob
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedKFold as kfold
import numpy as np
import torch
import torchvision.transforms as transforms
import os
from tqdm import tqdm
import torch.nn as nn
from torchsummary import summary

import copy
import tempfile
from pathlib import Path
from sklearn import metrics
import random
import math

from google.colab import drive
drive.mount('/content/drive')

root_folder="/content/drive/Shareddrives/Comp 540/Test_shared_Drive"

class MyNpydataset(Dataset):
    def __init__(self, imgs, labels, names, preprocess=None):
        """

        :param imgs: [N*1*W*D] or [N*W*D]
        :param labels: [N]
        :param names:  [N]
        :param preprocess:
        """
        self.imgs = imgs
        self.labels = labels
        self.names = names
        self.preprocess = preprocess

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, index):
        img = self.imgs[index]
        label = self.labels[index]

        if self.preprocess is not None:
            img = self.preprocess(img)

        return img, label

    def getLable(self, index):
        return self.labels[index]

    def getName(self, index):
        return self.names[index]


def getDataset(positive_npy_file, positive_names_npy_file, negetive_npy_file, negetive_names_npy_file):

    ################
    # read npy file
    ################
    positives=np.load(positive_npy_file, allow_pickle=True)[()]
    negatives=np.load(negetive_npy_file, allow_pickle=True)[()]
    positives_names=np.load(positive_names_npy_file, allow_pickle=True)[()]
    negatives_names=np.load(negetive_names_npy_file, allow_pickle=True)[()]


    # get all labels and all images
    all_labels = np.concatenate(
        (
            np.ones((positives.shape[0],)),
            np.zeros((negatives.shape[0],)),
        ),
        axis=0
    )
    all_imgs = np.vstack((positives, negatives))
    all_names = np.hstack((positives_names, negatives_names))
    all_indices = [i for i in range(all_imgs.shape[0])]

    ################
    # def norm
    ################
    def norm_transform(img, dataset_mean):
        img = img - dataset_mean
        return img.astype(np.float32) / max(np.abs(img.min()), img.max()) * 2-1.

    dataset_mean = np.mean(all_imgs)
    preprocess = transforms.Compose([
        lambda img: cv2.resize(img, (224,224)),
        lambda img: img[None, :, :] if len(img.shape) == 2 else img,  # 1*H*W
        lambda img: norm_transform(img, dataset_mean),
        lambda img: torch.from_numpy(img)
    ])

    ################
    # split
    ################
    train_val_indices, test_indices, train_val_labels, test_labels = train_test_split(all_indices,
                                                                                        all_labels,
                                                                                        test_size=0.2,
                                                                                        random_state=42,
                                                                                        stratify=all_labels)

    test_dataset = MyNpydataset(all_imgs[test_indices], test_labels, all_names[test_indices],
                                preprocess=preprocess)


    skf = kfold(n_splits=5, shuffle=True, random_state=42)
    cur_fold_index = 0
    for trainIndex, testIndex in skf.split(train_val_indices, train_val_labels):
        # get train/val imgs
        train_imgs = all_imgs[[train_val_indices[ix] for ix in trainIndex]]
        val_imgs = all_imgs[[train_val_indices[ix] for ix in testIndex]]

        # get train/val labels
        train_labels = all_labels[[train_val_indices[ix] for ix in trainIndex]]
        val_labels = all_labels[[train_val_indices[ix] for ix in testIndex]]

        # get train/val names
        train_names = all_names[[train_val_indices[ix] for ix in trainIndex]]
        val_names = all_names[[train_val_indices[ix] for ix in testIndex]]

        break

    ########## get dataset
    train_dataset = MyNpydataset(train_imgs, train_labels, train_names,
                                 preprocess=preprocess)
    val_dataset = MyNpydataset(val_imgs, val_labels, val_names,
                               preprocess=preprocess)

    print("train len:", train_dataset.__len__(), train_dataset.labels.sum()/len(train_dataset.labels))
    print("val len:", val_dataset.__len__(), val_dataset.labels.sum()/len(val_dataset.labels))
    print("test len:", test_dataset.__len__(), test_dataset.labels.sum()/len(test_dataset.labels))
    return train_dataset, val_dataset, test_dataset

positive_npy_file = f"{root_folder}/processed/new_2img_mask_npy/positive_imgs.npy"
positive_names_npy_file = f"{root_folder}/processed/new_2img_mask_npy/positive_imgs_names.npy"
negative_npy_file = f"{root_folder}/processed/new_2img_mask_npy/negative_imgs.npy"
negative_names_npy_file = f"{root_folder}/processed/new_2img_mask_npy/negative_imgs_names.npy"

np.load(positive_npy_file, allow_pickle=True)

np.load(negative_npy_file, allow_pickle=True)

train_set, val_set, test_set=getDataset(positive_npy_file, positive_names_npy_file, negative_npy_file, negative_names_npy_file)

train_labels = np.bincount(train_set.labels.astype(int))
val_labels = np.bincount(val_set.labels.astype(int))
test_labels = np.bincount(test_set.labels.astype(int))

train_labels = train_labels / sum(train_labels)
val_labels = val_labels / sum(val_labels)
test_labels = test_labels / sum(test_labels)

fig, axs = plt.subplots(1, 3, figsize=(18, 6))

axs[0].bar([0,1], train_labels)
axs[0].set_xticks([0, 1])
axs[0].set_title("training set")
axs[0].set_xlabel("Labels", fontsize=12)
axs[0].set_ylabel("Counts", fontsize=12)

axs[1].bar([0,1], val_labels)
axs[1].set_xticks([0, 1])
axs[1].set_title("validation set")
axs[1].set_xlabel("Labels", fontsize=12)
axs[1].set_ylabel("Counts", fontsize=12)

axs[2].bar([0,1], test_labels)
axs[2].set_xticks([0, 1])
axs[2].set_title("test set")
axs[1].set_xlabel("Labels", fontsize=12)
axs[1].set_ylabel("Counts", fontsize=12)

plt.show()

def count_params(model):
    pytorch_total_params = sum(p.numel() for p in model.parameters())
    print("Total:", pytorch_total_params)

    pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print("Trainable:", pytorch_total_params)


class CBR_Block(nn.Module):
    def __init__(self, in_channel, out_channel, filter_size):
        super().__init__()

        self.in_channel = in_channel
        self.out_channel = out_channel
        self.filter_size = filter_size

        self.conv = nn.Sequential(
            nn.Conv2d(in_channels=self.in_channel, out_channels=self.out_channel,
                      kernel_size=filter_size, stride=1),
            nn.BatchNorm2d(num_features=out_channel),
            # nn.ReLU() # can also change this to leaky ReLU,
            nn.LeakyReLU(negative_slope=0.01),
            nn.Dropout(p=0.2), #adding a dropout layer can prevent overfitting
        )

    def forward(self, x):
        return self.conv(x)



# can change the filter_size to maybe 3 or 5
class CBR_Model(nn.Module):
    def __init__(self, in_channel=1, in_feature_size=224,
                 out_channels=[32, 64, 128, 256, 512], filter_size=3, label_num=2):
        super().__init__()

        self.out_feature_size = in_feature_size  # for the global avg pool, calculate the out feature size of conv layers
        self.CBR_Blocks = nn.ModuleList()
        out_channel_ix = 0
        for out_channel in out_channels:
            self.CBR_Blocks.append(
                CBR_Block(in_channel=in_channel, out_channel=out_channel,
                          filter_size=filter_size)
            )
            self.out_feature_size = math.floor((self.out_feature_size - filter_size) / 1 + 1)  # (in+P-K)/S+1

            out_channel_ix += 1
            if out_channel_ix < 5:
                self.CBR_Blocks.append(nn.MaxPool2d(kernel_size=(3, 3), stride=(2, 2)))
                self.out_feature_size = math.floor((self.out_feature_size - 3) / 2 + 1)  # (in+P-K)/S+1
            in_channel = out_channel

        self.global_avgpool = nn.AvgPool2d(kernel_size=self.out_feature_size, stride=self.out_feature_size, padding=0)
        self.classifier = nn.Linear(out_channels[-1], label_num)

    def forward(self, x):
        for cbr_block in self.CBR_Blocks:
            x = cbr_block(x)

        assert self.out_feature_size == x.shape[
            -1], f"It's not a global avg pool. Avg pool size: {self.out_feature_size} while feature size:{x.shape[-1]}"
        x = self.global_avgpool(x)

        x = x.reshape(x.shape[0], -1)
        y = self.classifier(x)
        return y


################################################################################################
def getCBRModel(in_channel=1, in_feature_size=224,
                out_channels=[32, 64, 128, 256, 512], filter_size=7, label_num=2):
    model = CBR_Model(in_channel=in_channel, in_feature_size=in_feature_size,
                      out_channels=out_channels, filter_size=filter_size, label_num=label_num)

    return model

# test model
device=torch.device("cuda")
model = getCBRModel(in_channel=1, in_feature_size=224,
                    out_channels=[32, 64, 128, 256], filter_size=7, label_num=2).to(device)

summary(model, (1,224,224))
count_params(model)

def train_model_single(train_loader, val_loader, model,
                       cost_function, optimizer, scheduler, device):

    model = model
    cost_function = cost_function.to(device)

    loop = tqdm(train_loader, leave=True)
    model.train()
    cumulative_loss = 0
    correct = 0
    n_samples = 0

    for batch_idx, (x, y) in enumerate(loop):
        xb = x.float().to(device)
        yb = y.type(torch.LongTensor)
        yb = yb.to(device)

        # predict
        predicted = model(xb)
        loss = cost_function(predicted, yb)

        # back
        model.zero_grad()
        loss.backward()
        optimizer.step()

        ##########
        # stats
        ##########
        cumulative_loss += loss.item()
        # Count
        predicted_ = predicted.detach().softmax(dim=1)
        max_vals, max_ids = predicted_.max(dim=1)
        correct += (max_ids == yb).sum().cpu().item()
        n_samples += xb.size(0)
        n_batches = 1 + batch_idx
        # statistic
        avg_loss = cumulative_loss / n_batches
        avg_acc = correct / n_samples

        # tqdm print
        loop.set_postfix(loss=avg_loss, acc=avg_acc)

    loop.close()

    ##########
    # val
    ##########
    model.eval()
    loop_val = tqdm(val_loader, leave=True)
    cumulative_loss = 0
    correct = 0
    n_samples = 0
    with torch.no_grad():
        valPreds_for_auc = []
        valTrues = []
        for batch_idx, (x, y) in enumerate(loop_val):
            xb = x.float().to(device)
            yb = y.type(torch.LongTensor)
            yb = yb.to(device)

            # predict
            predicted = model(xb)
            loss = cost_function(predicted, yb)

            ##########
            # stats
            ##########
            cumulative_loss += loss.item()
            # Count
            predicted_ = predicted.detach().softmax(dim=1)
            valPreds_for_auc.append(predicted_.cpu())
            valTrues.append(yb.cpu().numpy())
            max_vals, max_ids = predicted_.max(dim=1)
            correct += (max_ids == yb).sum().cpu().item()
            n_samples += xb.size(0)
            n_batches = 1 + batch_idx
            # statistic
            val_avg_loss = cumulative_loss / n_batches
            val_avg_acc = correct / n_samples
            # tqdm print
            loop_val.set_postfix(val_loss=val_avg_loss, val_acc=val_avg_acc)

    loop_val.close()

    # get auc
    valPreds_for_auc = np.vstack(valPreds_for_auc)
    valTrues = np.hstack(valTrues)
    valTrues_for_auc = np.zeros((valTrues.size, 2))
    valTrues_for_auc[np.arange(valTrues.size), valTrues.astype('int')] = 1
    fpr = dict()
    tpr = dict()
    roc_auc = dict()
    for i in range(2):
        fpr[i], tpr[i], _ = metrics.roc_curve(valTrues_for_auc[:, i], valPreds_for_auc[:, i])
        roc_auc[i] = metrics.auc(fpr[i], tpr[i])
    val_auc = roc_auc[0]

    return avg_loss, avg_acc, val_avg_loss, val_avg_acc, val_auc


def train_model(epochs,train_loader, val_loader, model, cost_function, optimizer, scheduler,device):

    best_val_auc = 0

    avg_losses=[]
    avg_accs=[]
    val_avg_losses=[]
    val_avg_accs=[]
    val_aucs=[]
    for epoch in range(epochs):
        avg_loss, avg_acc, val_avg_loss, val_avg_acc, val_auc = train_model_single(train_loader, val_loader, model,
                                                                                   cost_function, optimizer, scheduler,
                                                                                   device)
        avg_losses.append(avg_loss)
        avg_accs.append(avg_acc)
        val_avg_losses.append(val_avg_loss)
        val_avg_accs.append(val_avg_acc)
        val_aucs.append(val_auc)

        # scheduler
        if scheduler != -1:
            scheduler.step()

        # save model
        if val_auc >= best_val_auc:
            best_val_auc = val_auc
            torch.save(model.state_dict(), "best_so_far.pth")

    plt.plot(range(len(avg_losses)), avg_losses, label="train_loss")
    plt.plot(range(len(val_avg_losses)), val_avg_losses, label="val_loss")
    plt.legend()
    plt.show()

    plt.plot(range(len(avg_accs)), avg_accs, label="train_acc")
    plt.plot(range(len(val_avg_accs)), val_avg_accs, label="val_acc")
    plt.legend()
    plt.show()

    print("best auc:", best_val_auc)

lr=1e-4
wd=1e-3
scheduler_ld=0.9
B=64

"""
dataloader
"""
train_loader = DataLoader(
    dataset=train_set,
    batch_size=B,
    num_workers=8,
    pin_memory=True,
    shuffle=True,
    drop_last=True,
)

val_loader = DataLoader(
    dataset=val_set,
    batch_size=B,
    num_workers=8,
    pin_memory=True,
    shuffle=True,
    drop_last=False,
)

"""
model
"""
device=torch.device("cuda")
model = getCBRModel(in_channel=1, in_feature_size=224,
                    out_channels=[32, 64, 128, 256], filter_size=7, label_num=2).to(device)

"""
cost function
"""
positive_ratio = train_set.labels.sum() / len(train_set.labels)
class_weights = [positive_ratio, 1 - positive_ratio]
print("class_weights:", class_weights)
cost_function = torch.nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float)).to(device)

"""
optimizer
"""
optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)
lambda1 = lambda epoch: scheduler_ld ** epoch
scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda1)

"""
train
"""
# change the epochs to 20 - 30
train_model(10,train_loader, val_loader, model, cost_function, optimizer, scheduler, device)

lr=5e-4
wd=1e-3
scheduler_ld=0.8
B=128

"""
dataloader
"""
train_loader = DataLoader(
    dataset=train_set,
    batch_size=B,
    num_workers=8,
    pin_memory=True,
    shuffle=True,
    drop_last=True,
)

val_loader = DataLoader(
    dataset=val_set,
    batch_size=B,
    num_workers=8,
    pin_memory=True,
    shuffle=True,
    drop_last=False,
)


"""
model
"""
device=torch.device("cuda")
model = getCBRModel(in_channel=1, in_feature_size=224,
                    out_channels=[32, 64, 128, 256], filter_size=7, label_num=2).to(device)

"""
cost function
"""
positive_ratio = train_set.labels.sum() / len(train_set.labels)
class_weights = [positive_ratio, 1 - positive_ratio]
print("class_weights:", class_weights)
cost_function = torch.nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float)).to(device)

"""
optimizer
"""
optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)
lambda1 = lambda epoch: scheduler_ld ** epoch
scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda1)

"""
train
"""
train_model(10,train_loader, val_loader, model, cost_function, optimizer, scheduler, device)

lr=5e-4
wd=1e-2
scheduler_ld=0.8
B=128

"""
dataloader
"""
train_loader = DataLoader(
    dataset=train_set,
    batch_size=B,
    num_workers=8,
    pin_memory=True,
    shuffle=True,
    drop_last=True,
)

val_loader = DataLoader(
    dataset=val_set,
    batch_size=B,
    num_workers=8,
    pin_memory=True,
    shuffle=True,
    drop_last=False,
)


"""
model
"""
device=torch.device("cuda")
model = getCBRModel(in_channel=1, in_feature_size=224,
                    out_channels=[32, 64, 128, 256], filter_size=7, label_num=2).to(device)

"""
cost function
"""
positive_ratio = train_set.labels.sum() / len(train_set.labels)
class_weights = [positive_ratio, 1 - positive_ratio]
print("class_weights:", class_weights)
cost_function = torch.nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float)).to(device)

"""
optimizer
"""
optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)
lambda1 = lambda epoch: scheduler_ld ** epoch
scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda1)

"""
train
"""
train_model(10,train_loader, val_loader, model, cost_function, optimizer, scheduler, device)

lr=1e-3
wd=1e-2
scheduler_ld=0.8
B=128

"""
dataloader
"""
train_loader = DataLoader(
    dataset=train_set,
    batch_size=B,
    num_workers=8,
    pin_memory=True,
    shuffle=True,
    drop_last=True,
)

val_loader = DataLoader(
    dataset=val_set,
    batch_size=B,
    num_workers=8,
    pin_memory=True,
    shuffle=True,
    drop_last=False,
)


"""
model
"""
device=torch.device("cuda")
model = getCBRModel(in_channel=1, in_feature_size=224,
                    out_channels=[32, 64, 128, 256], filter_size=7, label_num=2).to(device)

"""
cost function
"""
positive_ratio = train_set.labels.sum() / len(train_set.labels)
class_weights = [positive_ratio, 1 - positive_ratio]
print("class_weights:", class_weights)
cost_function = torch.nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float)).to(device)

"""
optimizer
"""
optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)
lambda1 = lambda epoch: scheduler_ld ** epoch
scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda1)

"""
train
"""
train_model(10,train_loader, val_loader, model, cost_function, optimizer, scheduler, device)

# Most optimal model

lr=5e-4
wd=1e-3
scheduler_ld=0.9
B=64

"""
dataloader
"""
train_loader = DataLoader(
    dataset=train_set,
    batch_size=B,
    num_workers=8,
    pin_memory=True,
    shuffle=True,
    drop_last=True,
)

val_loader = DataLoader(
    dataset=val_set,
    batch_size=B,
    num_workers=8,
    pin_memory=True,
    shuffle=True,
    drop_last=False,
)

"""
model
"""
device=torch.device("cuda")
model = getCBRModel(in_channel=1, in_feature_size=224,
                    out_channels=[32, 64, 128, 256], filter_size=7, label_num=2).to(device)

"""
cost function
"""
positive_ratio = train_set.labels.sum() / len(train_set.labels)
class_weights = [positive_ratio, 1 - positive_ratio]
print("class_weights:", class_weights)
cost_function = torch.nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float)).to(device)

"""
optimizer
"""
optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)
lambda1 = lambda epoch: scheduler_ld ** epoch
scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda1)

"""
train
"""
# change the epochs to 20 - 30
train_model(40,train_loader, val_loader, model, cost_function, optimizer, scheduler, device)

model = getCBRModel(in_channel=1, in_feature_size=224,
                    out_channels=[32, 64, 128, 256], filter_size=7, label_num=2).to(device)
model.load_state_dict(torch.load('best_so_far.pth'))

from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix

def evaluate_test_set(test_loader, model, cost_function, device):
    # Set model to evaluation mode
    model.eval()

    # Initialize variables for evaluation
    cumulative_loss = 0
    correct = 0
    n_samples = 0
    testPreds_for_auc = []
    y_test = []

    loop_test = tqdm(test_loader, leave=True)
    with torch.no_grad():
        for batch_idx, (x, y) in enumerate(loop_test):
            xb = x.float().to(device)
            if not isinstance(y, torch.Tensor):
              y = torch.tensor(y)
            yb = y.to(device, dtype=torch.long)

            # Predict
            predicted = model(xb)
            loss = cost_function(predicted, yb)

            # Collect statistics
            cumulative_loss += loss.item()

            predicted_ = predicted.detach().softmax(dim=1)
            testPreds_for_auc.append(predicted_.cpu().numpy())
            y_test.append(yb.cpu().numpy())

            max_vals, max_ids = predicted_.max(dim=1)
            correct += (max_ids == yb).sum().cpu().item()
            n_samples += xb.size(0)
            n_batches = 1 + batch_idx
            # tqdm print
            avg_loss = cumulative_loss / n_batches
            avg_acc = correct / n_samples
            loop_test.set_postfix(test_loss=avg_loss, test_acc=avg_acc)

    loop_test.close()

    # Prepare for AUC calculation
    testPreds_for_auc = np.vstack(testPreds_for_auc)
    y_test = np.hstack(y_test)
    test_auc = roc_auc_score(y_test, testPreds_for_auc[:, 1])

    # Calculate additional metrics
    y_pred = testPreds_for_auc.argmax(axis=1)
    #report = classification_report(testTrues, y_pred, digits=4)
    #cm = confusion_matrix(testTrues, y_pred)

    return y_test, y_pred


test_loader = DataLoader(
    dataset=test_set,
    batch_size=B,
    num_workers=8,
    pin_memory=True,
    shuffle=True,
    drop_last=True,
)

# Usage
model = getCBRModel(in_channel=1, in_feature_size=224,
                    out_channels=[32, 64, 128, 256], filter_size=7, label_num=2).to(device)
model.load_state_dict(torch.load('best_so_far.pth'))  # Load the trained weights
model.to('cuda' if torch.cuda.is_available() else 'cpu')  # Move model to device (GPU or CPU)
# Assuming test_loader is a DataLoader for the test set
y_test, y_pred = evaluate_test_set(test_loader, model, cost_function, device)

from sklearn.metrics import (
    roc_auc_score,
    classification_report,
    confusion_matrix,
    accuracy_score,
    precision_score,
    recall_score,
    f1_score
)


print('Accuracy Score: ' + str(accuracy_score(y_test, y_pred)))
print('Precision: ' + str(precision_score(y_test, y_pred, average='weighted', zero_division=0)))
print('Recall: '+ str(recall_score(y_test, y_pred, average='weighted')))
print('F1 Score: ' + str(f1_score(y_test, y_pred, average='weighted')))

cm = confusion_matrix(y_test, y_pred)

import seaborn as sns
class_names = ['Negative', 'Positive']
sns.heatmap(cm, annot=True, fmt='d', xticklabels=class_names, yticklabels=class_names, cbar=False)

# Add labels, title, and ticks
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.title('Confusion Matrix for CNN')
plt.show()

"""# ResNet-18"""

import torch
import torch.nn as nn
import torchvision.models as models
from torchsummary import summary

class ResNet18(nn.Module):
    def __init__(self, num_classes=2):
        super(ResNet18, self).__init__()

        # Load the pretrained ResNet18 model
        self.resnet = models.resnet18(pretrained=True)

        # Modify the first convolutional layer to handle 128x128 images (grayscale)
        self.resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)  # For grayscale

        # Modify the fully connected layer for binary classification
        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, num_classes)

    def forward(self, x):
        return self.resnet(x)

# Instantiate the model
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
resnet = ResNet18(num_classes=2).to(device)

# Print model summary
summary(resnet, (1, 128, 128))

# Most optimal model
lr=1e-4
wd=1e-3
scheduler_ld=0.9
B=64

"""
dataloader
"""
train_loader = DataLoader(
    dataset=train_set,
    batch_size=B,
    num_workers=8,
    pin_memory=True,
    shuffle=True,
    drop_last=True,
)

val_loader = DataLoader(
    dataset=val_set,
    batch_size=B,
    num_workers=8,
    pin_memory=True,
    shuffle=True,
    drop_last=False,
)

"""
model
"""
device=torch.device("cuda")
resnet = ResNet18(num_classes=2).to(device)

"""
cost function
"""
positive_ratio = train_set.labels.sum() / len(train_set.labels)
class_weights = [positive_ratio, 1 - positive_ratio]
print("class_weights:", class_weights)
cost_function = torch.nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float)).to(device)

"""
optimizer
"""
optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)
lambda1 = lambda epoch: scheduler_ld ** epoch
scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda1)

"""
train
"""
# change the epochs to 20 - 30
train_model(10,train_loader, val_loader, resnet, cost_function, optimizer, scheduler, device)

from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix

def evaluate_test_set(test_loader, model, cost_function, device):
    # Set model to evaluation mode
    model.eval()

    # Initialize variables for evaluation
    cumulative_loss = 0
    correct = 0
    n_samples = 0
    testPreds_for_auc = []
    y_test = []

    loop_test = tqdm(test_loader, leave=True)
    with torch.no_grad():
        for batch_idx, (x, y) in enumerate(loop_test):
            xb = x.float().to(device)
            if not isinstance(y, torch.Tensor):
              y = torch.tensor(y)
            yb = y.to(device, dtype=torch.long)

            # Predict
            predicted = model(xb)
            loss = cost_function(predicted, yb)

            # Collect statistics
            cumulative_loss += loss.item()

            predicted_ = predicted.detach().softmax(dim=1)
            testPreds_for_auc.append(predicted_.cpu().numpy())
            y_test.append(yb.cpu().numpy())

            max_vals, max_ids = predicted_.max(dim=1)
            correct += (max_ids == yb).sum().cpu().item()
            n_samples += xb.size(0)
            n_batches = 1 + batch_idx
            # tqdm print
            avg_loss = cumulative_loss / n_batches
            avg_acc = correct / n_samples
            loop_test.set_postfix(test_loss=avg_loss, test_acc=avg_acc)

    loop_test.close()

    # Prepare for AUC calculation
    testPreds_for_auc = np.vstack(testPreds_for_auc)
    y_test = np.hstack(y_test)
    test_auc = roc_auc_score(y_test, testPreds_for_auc[:, 1])

    # Calculate additional metrics
    y_pred = testPreds_for_auc.argmax(axis=1)
    #report = classification_report(testTrues, y_pred, digits=4)
    #cm = confusion_matrix(testTrues, y_pred)

    return y_test, y_pred


test_loader = DataLoader(
    dataset=test_set,
    batch_size=B,
    num_workers=8,
    pin_memory=True,
    shuffle=True,
    drop_last=True,
)

# Usage
resnet = ResNet18(num_classes=2).to(device)
resnet.load_state_dict(torch.load('best_so_far.pth'))  # Load the trained weights
resnet.to('cuda' if torch.cuda.is_available() else 'cpu')  # Move model to device (GPU or CPU)
# Assuming test_loader is a DataLoader for the test set
y_test, y_pred = evaluate_test_set(test_loader, resnet, cost_function, device)

from sklearn.metrics import (
    roc_auc_score,
    classification_report,
    confusion_matrix,
    accuracy_score,
    precision_score,
    recall_score,
    f1_score
)


print('Accuracy Score: ' + str(accuracy_score(y_test, y_pred)))
print('Precision: ' + str(precision_score(y_test, y_pred, average='weighted', zero_division=0)))
print('Recall: '+ str(recall_score(y_test, y_pred, average='weighted')))
print('F1 Score: ' + str(f1_score(y_test, y_pred, average='weighted')))

cm = confusion_matrix(y_test, y_pred)

import seaborn as sns
class_names = ['Negative', 'Positive']
sns.heatmap(cm, annot=True, fmt='d', xticklabels=class_names, yticklabels=class_names, cbar=False)

# Add labels, title, and ticks
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.title('Confusion Matrix for ResNet')
plt.show()

"""# DenseNet-121"""

import torch
import torch.nn as nn
import torchvision.models as models

class DenseNet121(nn.Module):
    def __init__(self, num_classes=2):
        super(DenseNet121, self).__init__()

        # Load the pretrained DenseNet121 model
        self.densenet = models.densenet121(pretrained=True)

        # Modify the first convolutional layer to handle grayscale (1 channel) images
        self.densenet.features.conv0 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

        # Replace the classifier layer to output the number of classes (binary classification)
        num_ftrs = self.densenet.classifier.in_features
        self.densenet.classifier = nn.Linear(num_ftrs, num_classes)

    def forward(self, x):
        return self.densenet(x)

# Instantiate the model
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
densenet = DenseNet121(num_classes=2).to(device)

densenet

lr=1e-4
wd=1e-3
scheduler_ld=0.9
B=64

"""
dataloader
"""
train_loader = DataLoader(
    dataset=train_set,
    batch_size=B,
    num_workers=8,
    pin_memory=True,
    shuffle=True,
    drop_last=True,
)

val_loader = DataLoader(
    dataset=val_set,
    batch_size=B,
    num_workers=8,
    pin_memory=True,
    shuffle=True,
    drop_last=False,
)

"""
model
"""
device=torch.device("cuda")
densenet = DenseNet121(num_classes=2).to(device)

"""
cost function
"""
positive_ratio = train_set.labels.sum() / len(train_set.labels)
class_weights = [positive_ratio, 1 - positive_ratio]
print("class_weights:", class_weights)
cost_function = torch.nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float)).to(device)

"""
optimizer
"""
optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)
lambda1 = lambda epoch: scheduler_ld ** epoch
scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda1)

"""
train
"""
# change the epochs to 20 - 30
train_model(10,train_loader, val_loader, densenet, cost_function, optimizer, scheduler, device)

import seaborn as sns
class_names = ['Negative', 'Positive']
sns.heatmap(cm, annot=True, fmt='d', xticklabels=class_names, yticklabels=class_names, cbar=False)

# Add labels, title, and ticks
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.title('Confusion Matrix for DenseNet')
plt.show()